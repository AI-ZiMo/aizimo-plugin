# LangChain 1.0 Message Types and Model Responses

This guide covers the structure and handling of different message types in LangChain 1.0, with a focus on understanding and working with AIMessage objects returned by models.

## Message Types Overview

LangChain 1.0 uses several message types to represent different parts of a conversation:

### Core Message Types

1. **AIMessage** - Responses generated by AI models
2. **HumanMessage** - Messages from users/humans
3. **SystemMessage** - System instructions and context
4. **FunctionMessage/ToolMessage** - Results from tool/function calls
5. **ChatMessage** - Generic message type with customizable role

## AIMessage Structure Deep Dive

An AIMessage object contains not just the content but also rich metadata about the model response. Here's a complete example:

```python
from langchain_core.messages import AIMessage

# Example AIMessage from DeepSeek model
ai_message = AIMessage(
    content='ä½ å¥½å‘€ï¼æˆ‘æ˜¯å°æ™ºï¼Œä¸€ä¸ªä¹äºŽåŠ©äººçš„æ™ºèƒ½åŠ©æ‰‹ã€‚æˆ‘çš„ä¸»è¦åŠŸèƒ½æ˜¯å›žç­”é—®é¢˜ã€æä¾›å»ºè®®ã€ååŠ©è§£å†³é—®é¢˜ï¼Œæˆ–è€…é™ªä½ èŠå¤©ã€‚æ— è®ºæ˜¯å­¦ä¹ ã€å·¥ä½œã€ç”Ÿæ´»çäº‹ï¼Œè¿˜æ˜¯æƒ³æ‰¾ç‚¹æœ‰è¶£çš„è¯é¢˜ï¼Œæˆ‘éƒ½å¯ä»¥å¸®å¿™ï¼å¦‚æžœæœ‰ä»»ä½•éœ€è¦ï¼Œéšæ—¶å‘Šè¯‰æˆ‘å“¦ï½ž ðŸ˜Š',
    additional_kwargs={'refusal': None},
    response_metadata={
        'token_usage': {
            'completion_tokens': 60,
            'prompt_tokens': 25,
            'total_tokens': 85,
            'completion_tokens_details': None,
            'prompt_tokens_details': {
                'audio_tokens': None,
                'cached_tokens': 0
            },
            'prompt_cache_hit_tokens': 0,
            'prompt_cache_miss_tokens': 25
        },
        'model_provider': 'deepseek',
        'model_name': 'deepseek-chat',
        'system_fingerprint': 'fp_ffc7281d48_prod0820_fp8_kvcache',
        'id': '311d2a0e-7fa2-4543-a8d6-374b201db854',
        'finish_reason': 'stop',
        'logprobs': None
    },
    id='lc_run--0d38df6a-0aa7-41fd-9b33-a9ecae16cb80-0',
    usage_metadata={
        'input_tokens': 25,
        'output_tokens': 60,
        'total_tokens': 85,
        'input_token_details': {'cache_read': 0},
        'output_token_details': {}
    }
)
```

### AIMessage Properties Explained

#### Core Properties
- **`content`**: The actual text response from the model
- **`id`**: Unique identifier for this specific message/run
- **`additional_kwargs`**: Additional provider-specific information
  - **`refusal`**: Content refusal information (usually None for normal responses)

#### Response Metadata
- **`response_metadata`**: Raw response data from the model provider
  - **`token_usage`**: Detailed token usage information
  - **`model_provider`**: The provider that generated the response (e.g., 'openai', 'deepseek')
  - **`model_name`**: Specific model used (e.g., 'gpt-4', 'deepseek-chat')
  - **`system_fingerprint`**: Model system fingerprint for reproducibility
  - **`finish_reason`**: Why the model stopped generating ('stop', 'length', 'content_filter')
  - **`id`**: Provider-specific response ID
  - **`logprobs`**: Log probabilities (if requested)

#### Usage Metadata
- **`usage_metadata`**: Standardized token usage across providers
  - **`input_tokens`**: Tokens in the input prompt
  - **`output_tokens`**: Tokens in the generated response
  - **`total_tokens`**: Sum of input and output tokens
  - **`input_token_details`**: Additional input token details (caching, etc.)
  - **`output_token_details`**: Additional output token details

## Working with AIMessage Objects

### Basic Access Patterns

```python
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage

# Create model and get response
model = ChatOpenAI(model="gpt-3.5-turbo")
response = model.invoke([HumanMessage(content="Hello, tell me a joke")])

# Access basic content
print(f"Response: {response.content}")

# Access metadata
print(f"Message ID: {response.id}")
print(f"Model used: {response.response_metadata.get('model_name')}")
print(f"Finish reason: {response.response_metadata.get('finish_reason')}")

# Access token usage
if response.usage_metadata:
    print(f"Total tokens: {response.usage_metadata['total_tokens']}")
    print(f"Input tokens: {response.usage_metadata['input_tokens']}")
    print(f"Output tokens: {response.usage_metadata['output_tokens']}")
```

### Cost Calculation

```python
def calculate_cost(message: AIMessage, provider: str = "openai") -> dict:
    """
    Calculate estimated cost based on token usage.

    Args:
        message: AIMessage object
        provider: Model provider for pricing

    Returns:
        Dictionary with cost breakdown
    """
    if not message.usage_metadata:
        return {"error": "No usage metadata available"}

    # Pricing per 1K tokens (example rates)
    pricing = {
        "openai": {
            "gpt-3.5-turbo": {"input": 0.0015, "output": 0.002},
            "gpt-4": {"input": 0.03, "output": 0.06}
        },
        "deepseek": {
            "deepseek-chat": {"input": 0.0001, "output": 0.0002}
        }
    }

    model_name = message.response_metadata.get("model_name", "gpt-3.5-turbo")
    provider_pricing = pricing.get(provider, {})
    model_pricing = provider_pricing.get(model_name, {"input": 0.001, "output": 0.002})

    input_tokens = message.usage_metadata.get("input_tokens", 0)
    output_tokens = message.usage_metadata.get("output_tokens", 0)

    input_cost = (input_tokens / 1000) * model_pricing["input"]
    output_cost = (output_tokens / 1000) * model_pricing["output"]
    total_cost = input_cost + output_cost

    return {
        "model": model_name,
        "input_tokens": input_tokens,
        "output_tokens": output_tokens,
        "input_cost": input_cost,
        "output_cost": output_cost,
        "total_cost": total_cost
    }

# Usage
cost_info = calculate_cost(response)
print(f"Estimated cost: ${cost_info['total_cost']:.6f}")
```

### Error Handling

```python
def safe_message_processing(message: AIMessage) -> dict:
    """
    Safely extract information from an AIMessage with proper error handling.
    """
    result = {
        "content": None,
        "token_usage": None,
        "model_info": None,
        "error": None
    }

    try:
        # Basic content
        result["content"] = message.content if message.content else ""

        # Token usage with fallback
        if message.usage_metadata:
            result["token_usage"] = {
                "total": message.usage_metadata.get("total_tokens", 0),
                "input": message.usage_metadata.get("input_tokens", 0),
                "output": message.usage_metadata.get("output_tokens", 0)
            }
        elif message.response_metadata.get("token_usage"):
            # Fallback to response_metadata for some providers
            usage = message.response_metadata["token_usage"]
            result["token_usage"] = {
                "total": usage.get("total_tokens", 0),
                "input": usage.get("prompt_tokens", 0),
                "output": usage.get("completion_tokens", 0)
            }

        # Model information
        result["model_info"] = {
            "provider": message.response_metadata.get("model_provider"),
            "model": message.response_metadata.get("model_name"),
            "finish_reason": message.response_metadata.get("finish_reason")
        }

    except Exception as e:
        result["error"] = f"Error processing message: {str(e)}"

    return result
```

### Streaming Responses

```python
async def stream_with_metadata(model, prompt: str):
    """
    Stream responses while collecting metadata.
    """
    collected_content = []

    async for chunk in model.astream(prompt):
        if hasattr(chunk, 'content') and chunk.content:
            collected_content.append(chunk.content)
            print(chunk.content, end="", flush=True)

    print()  # New line

    # Get the final complete message
    full_response = model.invoke(prompt)

    return {
        "content": "".join(collected_content),
        "metadata": safe_message_processing(full_response)
    }

# Usage
import asyncio
result = asyncio.run(stream_with_metadata(model, "Tell me a short story"))
```

## Message Serialization

### JSON Serialization

```python
import json
from datetime import datetime

def serialize_message(message: AIMessage) -> dict:
    """
    Convert AIMessage to JSON-serializable dictionary.
    """
    return {
        "content": message.content,
        "id": message.id,
        "type": "ai",
        "additional_kwargs": message.additional_kwargs,
        "response_metadata": message.response_metadata,
        "usage_metadata": message.usage_metadata,
        "timestamp": datetime.now().isoformat()
    }

def save_conversation(messages: list, filename: str):
    """
    Save conversation history to JSON file.
    """
    serialized = [serialize_message(msg) for msg in messages]

    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(serialized, f, indent=2, ensure_ascii=False)

# Usage
conversation = [
    HumanMessage(content="Hello!"),
    response  # AIMessage from model
]
save_conversation(conversation, "conversation.json")
```

## Provider-Specific Variations

### OpenAI Messages
```python
# OpenAI responses typically include:
{
    "response_metadata": {
        "token_usage": {
            "completion_tokens": 60,
            "prompt_tokens": 25,
            "total_tokens": 85
        },
        "model_name": "gpt-3.5-turbo",
        "finish_reason": "stop",
        "system_fingerprint": "fp_...",
        "logprobs": None
    }
}
```

### Anthropic Messages
```python
# Anthropic Claude responses:
{
    "response_metadata": {
        "token_usage": {
            "input_tokens": 25,
            "output_tokens": 60
        },
        "model_name": "claude-3-sonnet-20240229",
        "stop_reason": "end_turn",
        "stop_sequence": None
    }
}
```

### DeepSeek Messages
```python
# DeepSeek responses (as shown in the main example):
{
    "response_metadata": {
        "model_provider": "deepseek",
        "model_name": "deepseek-chat",
        "finish_reason": "stop",
        "system_fingerprint": "fp_ffc7281d48_prod0820_fp8_kvcache"
    }
}
```

## Best Practices

1. **Always check for metadata existence** - Not all providers provide the same metadata fields
2. **Handle token usage variations** - Use both `usage_metadata` and `response_metadata["token_usage"]`
3. **Monitor costs** - Track token usage across conversations
4. **Cache responses** - Use message IDs for deduplication
5. **Error handling** - Wrap metadata access in try-catch blocks
6. **Provider abstraction** - Create helper functions to normalize provider differences

## Common Patterns

### Response Analysis
```python
def analyze_response(message: AIMessage) -> dict:
    """
    Perform comprehensive analysis of model response.
    """
    analysis = {
        "length": len(message.content) if message.content else 0,
        "word_count": len(message.content.split()) if message.content else 0,
        "has_refusal": message.additional_kwargs.get("refusal") is not None,
        "finish_reason": message.response_metadata.get("finish_reason"),
        "model_provider": message.response_metadata.get("model_provider"),
        "token_efficiency": None
    }

    if message.usage_metadata:
        output_tokens = message.usage_metadata.get("output_tokens", 0)
        word_count = analysis["word_count"]
        if output_tokens > 0 and word_count > 0:
            analysis["token_efficiency"] = word_count / output_tokens

    return analysis
```

### Conversation Tracking
```python
class ConversationTracker:
    def __init__(self):
        self.messages = []
        self.total_tokens = 0
        self.total_cost = 0.0

    def add_message(self, message):
        self.messages.append(message)

        if message.usage_metadata:
            self.total_tokens += message.usage_metadata.get("total_tokens", 0)

    def get_stats(self):
        return {
            "message_count": len(self.messages),
            "total_tokens": self.total_tokens,
            "average_tokens_per_message": self.total_tokens / len(self.messages) if self.messages else 0
        }
```